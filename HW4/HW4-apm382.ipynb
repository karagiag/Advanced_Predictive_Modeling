{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"text-align: center;\">MIS 382N: Advanced Predictive Modeling</p>\n",
    "# <p style=\"text-align: center;\">Assignment 4</p>\n",
    "## <p style=\"text-align: center;\">Total points: 50 </p>\n",
    "## <p style=\"text-align: center;\">Due: Mon, November 14</p>\n",
    "\n",
    "\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. Please submit **only one** ipynb file from each group, and include the names of all the group members. Also, please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1 - MNIST (15 pts)\n",
    "In this question you classify hand-written digits. We want to use MNIST data set and try Multi-layer Perceptron (MLP) classifier using sklearn package in Python. In order to simplify the problem, we classify digits into 8 classes (digits 0,1,2,...7) and ignore records for digits 8 and 9.  Use the code below to access the data set and extract data with labels 0 to 7, and split the data set into train set and test set.\n",
    "\n",
    "1. Fit a Multilayer Perceptron Classifier using the standard options on sklearn's MLP on train data. Report the root MSE for both train and test data. (5 pts)\n",
    "\n",
    "    Use these parameters for your model: \n",
    "                    {hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, tol=1e-4, random_state=42,\n",
    "                    learning_rate_init=.1}\n",
    "                    \n",
    "2. To find better parameters for the MLP Classifier model, try an exhaustive search over all parameters of the data. Use sklearn's GridSearchCV to find the best subset of parameters from the set:\n",
    "                    { alpha = [0.1,0.01,0.001], activation : ['logistic', 'relu'] }\n",
    "    which parameters resulted in a more accurate model? Can you explain why? (5 pts)\n",
    "\n",
    "3. Select 5 misclassified images and display them. (5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import (train_test_split,KFold)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mnist = fetch_mldata(\"MNIST original\")\n",
    "# rescale the data, use the traditional train/test split\n",
    "X = (mnist.data / 255.)[:48200]\n",
    "y = mnist.target[:48200]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 2: Regression Trees (10 points)\n",
    "\n",
    "In this question, we will be exploring the application of regression tree (RT) using sklearn package in Python. You will be using the same Hitters.csv dataset (available on Canvas) used in HW2 Q5 to predict a baseball playerâ€™s Salary using all the 16 performance variables. Use a random state of 42 and a test size of 1/3 to split the data into training and test.\n",
    "\n",
    "1. Build a regression using [DecisionTreeRegressor](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html) with max_depth = 5. Report the mean squared errors on both training and test datasets. (4)\n",
    "2. Repeat Part-1 with max_depth = 2. (4)\n",
    "3. Briefly explain what you observe from these MSE values obtained by using maximum tree depths 5 and 2? Which tree is better and why? (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Question 3 - Support Vector Regression vs. Linear Regression (10pts)\n",
    "Download datasets 'summer_gpa_test.csv' and 'summer_gpa_train.csv' from Canvas. With this toy dataset, we want to predict GPA in Summer 2016 for students using 5 different features. So, target variable will be 'GPA_summer2016' in this problem.\n",
    "\n",
    "1. Fit a support vector regression using the default options on [sklearn's SVR](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) on training data. Note that the default kernel is \"rbf\".  Report the root MSE for both train and test data. (2)\n",
    "\n",
    "2. Fit SVR with 'linear' kernel and Linear Regression (for other options, use default parameter settings). Report RMSE of prediction on train and test data for the two methods. (3)\n",
    "\n",
    "3. Now, compare the results of three different methods, then provide a possible reason for SVR with RBF kernel not working well on test set. (1)\n",
    "\n",
    "4.  Provide simple residual plots on Train and Test set for all three methods. Specifically, submit a scatter plot wherein y-axis shows the residuals and x-axis the predicted values. What can you learn about the effect of outliers on different models from this problem? (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "d_train = pd.read_csv('summer_gpa_train.csv',index_col='student')\n",
    "d_test = pd.read_csv('summer_gpa_test.csv',index_col='student')\n",
    "\n",
    "y_train = d_train['GPA_summer2016']\n",
    "y_test = d_test['GPA_summer2016']\n",
    "x_train = d_train.drop('GPA_summer2016',axis=1)\n",
    "x_test = d_test.drop('GPA_summer2016',axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4: Bayesian Networks (10 pts)\n",
    "#### Part (a) (5 points)\n",
    "Consider the following Bayesian network of binary (True/False) variables.\n",
    "\n",
    "<img src=\"hw4q4pic1.png\">\n",
    "\n",
    "This is equivalent to saying that X and Y are conditionally independent given C, or P(X,Y|C) = P(X|C)P(Y|C). This happens to be the assumption used by the Naive Bayes classifier.\n",
    "\n",
    "The exact probabilities are given:  \n",
    "P(X = True | C = True) = .75  \n",
    "P(X = True | C = False) = .5  \n",
    "P(Y = True | C = True) = .25  \n",
    "P(Y = True | C = False) = .5  \n",
    "P(C = True) = .5\n",
    "\n",
    "Find P(C = True | X = True, Y = True). The easiest method is to use Bayes rule, along with the conditional independence equation given above.\n",
    "\n",
    "#### (b) (5 points)\n",
    "A new feature Z is added, and based on prior knowledge, we believe that one of the two networks given below properly captures the dependencies among the variables. Our goal is to determine P(C|X,Y,Z). For each of these two different networks:\n",
    "\n",
    "<img src=\"hw4q4pic2.png\">\n",
    "\n",
    "will P(C|X,Y,Z) be the same as P(C|X,Y) (the inference from part a)? Or will it be different? Give a separate answer for each network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5: Mulitclass Classification (5 points)\n",
    "\n",
    "One way of using a binary classifier for addressing a multiclass classification problem is to use a One-vs.-All (or One-vs.-Rest)  strategy.\n",
    "\n",
    "1. Briefly describe the  One-vs.-All method.  (2pts)\n",
    "\n",
    "2. What are two disadvantages of using the One-vs.-All method in situations where the number of classes $N$ is very large? (3pts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
